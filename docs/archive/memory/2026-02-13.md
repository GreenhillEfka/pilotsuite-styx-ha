# 2026-02-14 - Memory Update

## Ollama - EXTERNER SERVER ✅

### Konfiguration
- **Host:** `192.168.31.84:11434` (externer Ollama Server)
- **OLLAMA_HOST:** `192.168.31.84:11434`
- **Start-Skript:** `/config/.ollama/start-ollama.sh`

### Verfügbare Modelle (extern)
1. `glm-5:cloud` - **PRIMARY** (beste Qualität)
2. `deepseek-r1:latest` - Fallback
3. `codellama:latest` - Fallback

### Warum extern?
- ✅ Immer verfügbar (keine Persistenz-Probleme)
- ✅ Alle Modelle vorhanden
- ✅ Kein lokales Binary nötig
- ✅ Kein Update-Overhead

### Lokale Ollama Binary
- **Location:** `/config/.ollama/bin/ollama` (39MB, v0.16.1)
- **Status:** Backup, wird nicht mehr primär genutzt

## Priority Chain (Stand 2026-02-14)
```
1. ✅ glm-5:cloud (extern, beste Qualität) - PRIMARY
2. deepseek-r1:latest (extern)
3. codellama:latest (extern)
4. openrouter/auto (Cloud Fallback)
```

## Coding Agents Status

| Agent | Status | Notes |
|-------|--------|-------|
| Codex CLI | ✅ Works | gpt-5.2-codex, OpenAI API key |
| Gemini CLI | ✅ Works | OAuth, v0.28.2 |
| Claude CLI | ⚠️ Local only | OAuth works, exec hangs |
| Ollama | ✅ Extern | 192.168.31.84:11434 |

## Skills Created

- `/config/.openclaw/workspace/skills/gemini-expert/`
- `/config/.openclaw/workspace/skills/claude-code-expert/`
- `/config/.openclaw/workspace/skills/codex-expert/`
